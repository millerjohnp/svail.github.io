<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-69560860-3', 'auto');
  ga('send', 'pageview');

</script>

    <title>Silicon Valley AI Lab by svail</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Silicon Valley AI Lab</h1>
          <h2>Baidu Research - <a href="http://research.baidu.com">research.baidu.com</a></h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/svail" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <hr>

        <section id="main_content">

<h3> 
<a href="http://svail.github.io/GloballyNormalizedReader/" Onclick="_gaq.push(['trackEvent','Outbound’,'GloballyNormalizedReader',’svail.github.io/GloballyNormalizedReader',true]);" >Globally Normalized Reader</a>
</h3>
<p><strong>Date:</strong> August 23, 2017<br>
<strong>Authors:</strong> Jonathan Raiman, John Miller<br>
We present the Globally Normalized Reader - an approach to extractive question answering with the same performance and significantly lower computational complexity than previous methods. Many popular models like Bidirectional Attention Flow use expensive attention mechanisms, and others like the Match-LSTM explicitly score all possible answers. In constrast, the GNR casts question answering as a search problem and applies a learning to search framework to solve it. The resulting model is 20x faster than Bi-Directional Attention flow and is the 2nd highest ranked model on the Stanford Question Answering (SQUAD) dev set.          

<h3> 
<a href="http://svail.github.io/DeepBench-update/" Onclick="_gaq.push(['trackEvent','Outbound’,'DeepBench-update',’svail.github.io/DeepBench-update',true]);" >DeepBench updates with a focus on deep learning inference</a>
</h3>
<p><strong>Date:</strong> June 28, 2017<br>
<strong>Authors:</strong> Sharan Narang, Greg Diamos<br>  
In September 2016, we released DeepBench, an open source benchmarking tool that measures the performance of basic operations involved in training deep learning networks. The benchmark included results on several different processors used for training. We’ve extended DeepBench to include support for deep learning inference. Inference changes include using low precision kernels, modifying the sizes of kernels and benchmarking sparse operations. In addition, the training benchmark now has support for Gated Recurrent Units (GRU) and low precision training.         
          
<h3> 
<a href="http://svail.github.io/DeepBench/" Onclick="_gaq.push(['trackEvent','Outbound’,'DeepBench',’svail.github.io/DeepBench',true]);" >DeepBench: Open-Source Tool for benchmarking DL operations</a>
</h3>
<p><strong>Date:</strong> September 26, 2016<br>
<strong>Authors:</strong> Sharan Narang<br>  
DeepBench is the first open source benchmarking tool for evaluation the performance of deep learning operations on different hardware platforms.  DeepBench also includes a list of operations and workloads that are important to the performance of deep learning training. We’ve measured performance of matrix multiplies, convolutions,  Recurrent ops (vanilla RNNs and LSTMs) and all-reduce for different sizes and parameters on Nvidia and Intel processors. We welcome contributions from the deep learning community to add to the list of existing operations and from hardware vendors who would like provide benchmark          
          
<h3>
<a href="http://svail.github.io/diff_graphs/" Onclick="_gaq.push(['trackEvent','Outbound','persistent_rnns','svail.github.io/diff_graphs',true]);" >Optimizing RNNs with Differentiable Graphs</a>
</h3>
<p><strong>Part II:</strong> Optimizing RNN performance <br>
<strong>Date:</strong> June 14th, 2016<br>
<strong>Authors:</strong> Jesse Engel<br> 
Differentiable graph notation provides an easy way to visually infer the gradients for complex neural networks. We also show several useful rules of thumb for optimizing graphs of new algorithms.
 
<h3>
<a href="http://svail.github.io/persistent_rnns/" Onclick="_gaq.push(['trackEvent','Outbound','persistent_rnns','svail.github.io/persistent_rnns',true]);" >Persistent RNNs: 30 times faster RNN layers at small mini-batch sizes</a>
</h3>

<p><strong>Date:</strong> March 25th, 2016<br>
<strong>Authors:</strong> Greg Diamos<br> 
<strong>YouTube:</strong> <a href="https://youtu.be/JkXbTOt_JxE" Onclick="_gaq.push(['trackEvent','Outbound','YoutubePersistent','https://youtu.be/IFPwMKbdQnI',true]);" >SVAIL Tech Notes: Accelerating RNNs by Stashing Weights On-Chip</a><br>
At SVAIL, our mission is to create AI technology that lets us have a significant impact on hundreds of millions of people. We believe that a good way to do this is to improve the accuracy of speech recognition by scaling up deep learning algorithms on larger datasets than what has been done in the past.
          
<h3>
<a href="https://svail.github.io/mandarin" Onclick="_gaq.push(['trackEvent','Outbound','Mandarin','svail.github.io/mandarin',true]);" >Around the World in 60 Days: Getting Deep Speech to Work on Mandarin</a>
</h3>

<p><strong>Date:</strong> February 9th, 2016<br>
<strong>Authors:</strong> Tony Han, Ryan Prenger<br> 
<strong>YouTube:</strong> <a href="https://youtu.be/IFPwMKbdQnI" Onclick="_gaq.push(['trackEvent','Outbound','YoutubeMandarin','https://youtu.be/IFPwMKbdQnI',true]);" >SVAIL Tech Notes: Recognizing both English and Mandarin</a><br>
In our recent paper <a href="http://arxiv.org/abs/1512.02595" Onclick="_gaq.push(['trackEvent','Outbound','DeepSpeech2','arxiv.org/abs/1512.02595',true]);" >Deep Speech 2</a>, we showed our results in Mandarin. In just a few months, we had produced a Mandarin speech recognition system with a recognition rate better than native Mandarin speakers. Here we want to discuss what we did to adapt the system to Mandarin and how the end-to-end learning approach made the whole project easier. </p> 
       
          
<h3>
<a href="https://github.com/baidu-research/warp-ctc" Onclick="_gaq.push(['trackEvent','Outbound','WarpCTC','github.com/baidu-research/warp-ctc/',true]);" >Fast Open Source CPU/GPU Implementation of CTC</a>
</h3>

<p><strong>Date:</strong> January 14th, 2016<br>
<strong>Contact:</strong> svail-questions@baidu.com<br>
<strong>YouTube:</strong> <a href="https://youtu.be/JHxG-XPsJHs" Onclick="_gaq.push(['trackEvent','Outbound','YoutubeWarpCTC','https://youtu.be/JHxG-XPsJHs',true]);" >SVAIL Tech Notes: Warp CTC</a><br>
Warp-CTC from Baidu Research's Silicon Valley AI Lab is a fast parallel implementation of CTC, on both CPU and GPU. Warp-CTC can be used to solve supervised problems that map an input sequence to an output sequence, such as speech recognition. To get Warp-CTC follow the link above. If you are interested in integrating Warp-CTC into a machine learning framework reach out to us. We are happy to accept pull requests. </p> 
       
          
<h3>
<a id="investigating-performance-of-gpu-blas-libraries" class="anchor" href="#investigating-performance-of-gpu-blas-libraries" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="http://svail.github.io/rnn_perf/">Investigating performance of GPU BLAS Libraries</a>
</h3>

<p><strong>Part I:</strong> Optimizing RNN performance<br>
<strong>Date:</strong> November 17th, 2015<br>
<strong>Author:</strong> Erich Elsen<br>
Most researchers engaging in Neural Network research have been using GPUs for training for some time now due to 
the speed advantage they have over CPUs. GPUs from NVIDIA are almost universally preferred because they come with 
high quality BLAS (cuBLAS) and convolution (cuDNN) libraries. </p>
        
 &nbsp;

 </section>

        <footer>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.
        </footer>



      </div>
    </div>
  </body>
</html>
